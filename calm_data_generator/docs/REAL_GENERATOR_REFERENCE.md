# RealGenerator - Complete Reference

**Location:** `calm_data_generator.generators.tabular.RealGenerator`

The main generator for tabular data synthesis from real datasets.

---

## Initialization

```python
from calm_data_generator.generators.tabular import RealGenerator

gen = RealGenerator(
    auto_report=True,       # Automatically generate report after synthesis
    minimal_report=False,   # If True, faster report without correlations/PCA
    random_state=42,        # Seed for reproducibility
    logger=None,            # Optional custom Python logger
)
```

### Constructor Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `auto_report` | bool | `True` | Automatically generate a quality report |
| `minimal_report` | bool | `False` | Simplified report (faster) |
| `random_state` | int | `None` | Seed for reproducibility |
| `logger` | Logger | `None` | Custom Python Logger instance |

---

## Main Method: `generate()`

```python
synthetic_df = gen.generate(
    data=df,                          # Original DataFrame (required)
    n_samples=1000,                   # Number of samples to generate (required)
    method="ctgan",                   # Synthesis method
    target_col="target",              # Target column (optional)
    output_dir="./output",            # Output directory
    generator_name="my_generator",    # Base name for output files
    save_dataset=False,               # Save resulting CSV
    # Model parameters
    model_params={...},               # Method-specific parameters
    # Custom distributions
    custom_distributions={"target": {0: 0.3, 1: 0.7}},
    # Date injection
    date_col="date",
    date_start="2024-01-01",
    date_step={"days": 1},
    # Post-processing
    drift_injection_config=[...],
    dynamics_config={...},
    constraints=[...],
    adversarial_validation=True,      # Activate adversarial validation
)
```

### `generate()` Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `data` | DataFrame | - | Original and reference dataset (required) |
| `n_samples` | int | - | Number of samples to generate (required) |
| `method` | str | `"cart"` | Synthesis method to use |
| `target_col` | str | `None` | Target variable column name for balancing |
| `output_dir` | str | `None` | Directory to save reports and datasets |
| `generator_name` | str | `"RealGenerator"` | Base name for output files |
| `save_dataset` | bool | `False` | Whether to save the generated dataset to a CSV file |
| `custom_distributions` | Dict | `None` | Force specific distributions for columns |
| `date_col` | str | `None` | Name of the date column to inject |
| `date_start` | str | `None` | Start date ("YYYY-MM-DD") |
| `date_step` | Dict | `None` | Time increment (e.g., `{"days": 1}`) |
| `date_every` | int | `1` | Increment date every N rows |
| `drift_injection_config` | List[Dict] | `None` | Configuration for post-generation drift injection |
| `dynamics_config` | Dict | `None` | Configuration for dynamic feature evolution |
| `model_params` | Dict | `None` | Model-specific hyperparameters (passed as `**kwargs`) |
| `constraints` | List[Dict] | `None` | Integrity constraints |
| `adversarial_validation` | bool | `False` | Activate Discriminator Report (Real vs Synthetic) |

---

## Full `model_params` Reference

The `model_params` dictionary allows fine-tuning internal parameters for each synthesis method.

### Deep Learning (Synthcity)

| Parameter | Methods | Description |
|-----------|---------|-------------|
| `epochs` | `ctgan`, `tvae` | Number of training epochs (default: 300) |
| `batch_size` | `ctgan`, `tvae` | Training batch size (default: 500) |
| `n_units_conditional` | `ctgan`, `tvae` | Number of units in conditional layers |
| `n_units_in` | `ctgan`, `tvae` | Number of units in input layers |
| `lr` | `ctgan`, `tvae` | Learning rate |

**Example:**
```python
gen.generate(
    df, 1000, 
    method="ctgan",
    epochs=500, 
    batch_size=256
)
```


### CART (Decision Trees)

| Parameter | Description |
|-----------|-------------|
| `iterations` | Number of FCS iterations (default: 10) |
| `min_samples_leaf` | Minimum samples per leaf (auto if None) |
| `**kwargs` | Any parameter supported by sklearn's DecisionTree |

**Example:**
```python
model_params={"iterations": 20, "min_samples_leaf": 5, "max_depth": 10}
```

### Random Forest

| Parameter | Description |
|-----------|-------------|
| `iterations` | Number of FCS iterations (default: 10) |
| `n_estimators` | Number of trees |
| `min_samples_leaf` | Minimum samples per leaf |
| `**kwargs` | Any parameter supported by sklearn's RandomForest |

**Example:**
```python
model_params={"n_estimators": 100, "min_samples_leaf": 3, "max_depth": 15}
```

### LightGBM

| Parameter | Description |
|-----------|-------------|
| `iterations` | Number of FCS iterations (default: 10) |
| `n_estimators` | Number of boosting rounds |
| `learning_rate` | Learning rate |
| `**kwargs` | Any parameter supported by LightGBM |

**Example:**
```python
model_params={"n_estimators": 200, "learning_rate": 0.05, "max_depth": 8}
```

### Gaussian Mixture Models

| Parameter | Description |
|-----------|-------------|
| `n_components` | Number of Gaussian components (default: 5) |
| `covariance_type` | Covariance type: "full", "tied", "diag", "spherical" (default: "full") |
| `**kwargs` | Any parameter supported by sklearn's GaussianMixture |

**Example:**
```python
model_params={"n_components": 10, "covariance_type": "diag"}
```

### SMOTE / ADASYN

| Parameter | Description |
|-----------|-------------|
| `k_neighbors` | Number of k-NN neighbors for SMOTE (default: 5) |
| `n_neighbors` | Number of k-NN neighbors for ADASYN (default: 5) |
| `**kwargs` | Any parameter supported by imbalanced-learn's SMOTE/ADASYN |

**Example:**
```python
model_params={"k_neighbors": 7}  # For SMOTE
model_params={"n_neighbors": 5}  # For ADASYN
```

### Differential Privacy

| Parameter | Description |
|-----------|-------------|
| `epsilon` | Privacy Œµ parameter (default: 1.0, lower = more private) |
| `delta` | Privacy Œ¥ parameter (default: 1e-5) |
| `synth_type` | Synthesizer type: "dpgan", "pategan", "adsgan" (default: "dpgan") |

**Example:**
```python
model_params={"epsilon": 0.5, "delta": 1e-6, "synth_type": "dpgan"}
```

### DataSynthesizer

| Parameter | Description |
|-----------|-------------|
| `k` | Bayesian network degree (default: 5, k=1 is independent) |

**Example:**
```python
model_params={"k": 3}  # Higher k captures more correlations
```

### Diffusion Models

| Parameter | Default | Description |
|-----------|---------|-------------|
| `diffusion_steps` | 50 | Number of diffusion steps (higher = better quality, slower) |

**Example:**
```python
model_params={"diffusion_steps": 100}
```

---

## Available Synthesis Methods

### Deep Learning

| Method | Description | Key Parameters | Dependencies |
|--------|-------------|----------------|--------------|
| `ctgan` | Conditional Tabular GAN | `epochs`, `batch_size`, `lr` | Synthcity |
| `tvae` | Tabular Variational Autoencoder | `epochs`, `batch_size` | Synthcity |
| `diffusion` | Tabular Diffusion Models | `steps` | PyTorch |

### Statistical Models

| Method | Description | Key Parameters |
|--------|-------------|----------------|
| `gmm` | Gaussian Mixture Model | `gmm_n_components`, `gmm_covariance_type` |
| `copula` | Copula | Copula-based synthesis | Base installation |

### Fully Conditional Specification (FCS)

| Method | Description | Key Parameters |
|--------|-------------|----------------|
| `cart` | Decision Trees | `min_samples_leaf`, `iterations` |
| `rf` | Random Forest | `n_estimators`, `min_samples_leaf`, `iterations` |
| `lgbm` | LightGBM | `n_estimators`, `learning_rate`, `iterations` |

### Oversampling

| Method | Description | Key Parameters |
|--------|-------------|----------------|
| `smote` | Classic SMOTE | `n_neighbors` |
| `adasyn` | Adaptive SMOTE | `n_neighbors` |
| `resample` | Simple Bootstrap | - |

### Privacy

| Method | Description | Key Parameters |
|--------|-------------|----------------|
| `dp` | Differential Privacy | `epsilon`, `delta` |
| `datasynth` | DataSynthesizer | `k` |


### Single-Cell / High-Dimensional

| Method | Description | Key Parameters | Dependencies |
|--------|-------------|----------------|--------------|
| `scvi` | scVI Variational Autoencoder | `epochs`, `n_latent`, `n_layers` | scvi-tools |
|

---

## Method Selection Guide

Choose the right method based on your data and requirements:

| Use Case | Recommended Methods | Why |
|----------|---------------------|-----|
| **General tabular data** | `ctgan`, `tvae` | Best balance of quality and speed |
| **Small datasets (< 1000 rows)** | `cart`, `rf`, `gmm` | Don't overfit, fast |
| **Large datasets (> 100k rows)** | `lgbm`, `ctgan` | Scalable |
| **Preserve correlations** | `ctgan`| Capture feature relationships |
| **Class imbalance** | `smote`, `adasyn` | Designed for oversampling |
| **Fast prototyping** | `resample`, `cart` | Instant results |

| **Numeric-only data** | `gmm`, `diffusion` | Simple distributions |

### Single-Cell Methods Details

#### scVI (Single-cell Variational Inference)

Best for generating new single-cell-like observations from scratch. These methods are specifically designed for high-dimensional **transcriptomic data (RNA-seq)**. They use deep generative models to represent biological variation while handling the heavy sparsity and technical noise (dropout) typical of single-cell datasets. They are excellent for correcting "batch effects" and synthesizing coherent gene expression profiles.

**Input Format:** Accepts `pd.DataFrame`, `AnnData` objects, or strings (paths to `.h5`, `.h5ad`, or `.csv` files) directly.

**Using File Paths (H5/H5AD/CSV):**
```python
# Pass the file path directly - the generator loads it for you!
synthetic = gen.generate(
    data="path/to/data.csv",  # Or .h5ad, .h5
    n_samples=1000,
    method="scvi",
    target_col="cell_type"
)
```


**DataFrame Input:**
```python
synthetic = gen.generate(
    data=expression_df,      # Rows=cells, Columns=genes
    n_samples=1000,
    method="scvi",
    target_col="cell_type",  # Optional metadata column
    model_params={
        "epochs": 100,
        "n_latent": 10,      # Latent space dimensions
        "n_layers": 1,       # Encoder/decoder depth
    }
)
```

**AnnData Input (Recommended for single-cell data):**
```python
import anndata

# Create or load AnnData object
adata = anndata.read_h5ad("single_cell_data.h5ad")

synthetic = gen.generate(
    data=adata,              # Pass AnnData directly
    n_samples=1000,
    method="scvi",
    target_col="cell_type",  # Must be in adata.obs
    model_params={
        "epochs": 100,
        "n_latent": 10,
        "n_layers": 1,
    }
)
# Returns pd.DataFrame with gene columns + metadata
```

| Parameter | Default | Description |
|-----------|---------|-------------|
| `epochs` | 100 | Training epochs |
| `n_latent` | 10 | Latent space dimensionality |
| `n_layers` | 1 | Number of hidden layers |


> **AnnData Support:** When passing `AnnData`, the object is used directly without conversion, preserving the original structure. The output is always a `pd.DataFrame` containing both the gene expression and the observations metadata.



| Parameter | Default | Description |
|-----------|---------|-------------|
| `epochs` | 100 | Training epochs |
| `n_latent` | 10 | Latent space dimensionality |
| `condition_col` | None | Column with condition/batch labels (required) |
| `noise_scale` | 0.1 | Noise for sample diversity |

.

---

## Advanced Features

### Custom Distributions

```python
synthetic = gen.generate(
    data=df,
    n_samples=5000,
    method="ctgan",
    target_col="Churn",
    custom_distributions={
        "Churn": {0: 0.5, 1: 0.5},
        "Region": {"North": 0.4, "South": 0.6},
    }
)
```

### Date Injection

```python
synthetic = gen.generate(
    data=df,
    n_samples=1000,
    method="cart",
    date_col="timestamp",
    date_start="2024-06-01",
    date_step={"hours": 1},
    date_every=1,
)
```

### Post-Generation Drift

```python
synthetic = gen.generate(
    data=df,
    n_samples=1000,
    method="tvae",
    drift_injection_config=[
        {
            "method": "inject_feature_drift",
            "feature_cols": ["price"],
            "drift_type": "shift",
            "drift_magnitude": 0.3,
            "start_index": 500,
        }
    ],
)
```

---

## Automatic Reports

When `auto_report=True`, the following are generated:

- `quality_report.html`: Data profiling report
- `comparison_report.html`: Real vs. Synthetic comparison
- `quality_scores.json`: Detailed quality metrics
- `distribution_plots.png`: Distribution visualizations
- `correlation_heatmap.png`: Correlation maps

---

## Best Practices

6. **Severe imbalance:** Use `smote` or `adasyn` with `target_col`.

---

## Comprehensive Use Cases

### Case 1: Fraud Detection (Imbalanced Data)

**Scenario:** You have a dataset of credit card transactions where only 0.1% are fraudulent. You want to train a model but need more fraud cases.

**Solution:** Use `smote` or `ctgan` with forced distribution.

```python
from calm_data_generator.generators.tabular import RealGenerator

gen = RealGenerator()

# Option A: SMOTE (Oversampling)
synthetic_fraud = gen.generate(
    data=df,
    n_samples=5000,
    method="smote",
    target_col="is_fraud",
    model_params={"smote_neighbors": 5}
)

# Option B: CTGAN with Controlled Sampling
synthetic_balanced = gen.generate(
    data=df,
    n_samples=5000,
    method="ctgan",
    target_col="is_fraud",
    custom_distributions={"is_fraud": {0: 0.5, 1: 0.5}}, # Force 50/50 split
    model_params={"epochs": 500}
)
```




### Case 3: High-Performance Upsampling for Large Datasets

**Scenario:** You have a 1M row dataset and need 5M rows for stress testing a database. Deep learning methods are too slow.

**Solution:** Use `lgbm` (LightGBM) or `rf` (Random Forest) which are faster and scalable.

```python
synthetic_large = gen.generate(
    data=large_df,
    n_samples=5_000_000,
    method="lgbm",
    model_params={
        "lgbm_n_estimators": 100,
        "lgbm_learning_rate": 0.1
    },
    minimal_report=True  # Skip heavy reporting to save time
)
```

---

## Common Usage Scenarios (Quick Guide)

### 1. Time Series (Sequences)
For time series data, use standard tabular methods (CTGAN, TVAE, etc.) on properly structured temporal data.
*   **Future Forecasting:** Use `StreamGenerator` for infinite flows or manual date injection.

### 2. Classification and Regression (Supervised)
If you have a `target` column (e.g., price, churn) and the relationship $X \rightarrow Y$ is critical:
*   Use `method="lgbm"` (LightGBM) or `method="rf"` (Random Forest).
*   Always specify `target_col="column_name"`.
    ```python
    # The generator automatically detects if it's Regression or Classification
    gen.generate(data, target_col="price", method="lgbm") 
    ```

### 3. Clustering (Unsupervised)
If there's no clear target and you want to preserve natural data clusters:
*   Use `method="gmm"` (Gaussian Mixture Models) or `method="tvae"` (Variational Autoencoder).
    ```python
    gen.generate(data, method="tvae")
    ```

### 4. Multi-Label (Multiple Labels)
If a cell contains multiple values (e.g., `["A", "B", "C"]`) or string format `"A,B,C"`:
*   **Limitation:** Standard models don't handle lists within cells well.
*   **Solution:** Transform the column to **One-Hot Encoding** (multiple binary columns `is_A`, `is_B`) before passing to the generator. Tree-based models (`lgbm`, `cart`) will learn correlations between labels (e.g., if `is_A=1` often implies `is_B=1`).

### 5. Block Data (Partitioned Data)
If your data is logically fragmented (e.g., by Stores, Countries, Patients) and you want independent models for each:
*   Use **`RealBlockGenerator`** instead of `RealGenerator`.
    ```python
    block_gen = RealBlockGenerator()
    block_gen.generate(data, block_column="StoreID", method="cart") 
    ```
    *This trains a different model for each StoreID.*

### 6. Handling Imbalanced Data
If your target column has minority classes that you want to amplify:
*   **Automatic Balancing:** Use `balance_target=True`. The generator applies internal oversampling (SMOTE/RandomOverSampler) so the model learns equally from all classes.
    ```python
    gen.generate(data, target_col="fraud", balance_target=True, method="cart")
    ```
*   **Custom Distribution:** If you want a specific ratio (e.g., 70% Class A, 30% Class B):
    ```python
    gen.generate(data, target_col="status", custom_distributions={"status": {"Low": 0.7, "High": 0.3}})
    ```
    *Note: `balance_target=True` is a shortcut for `custom_distributions={"col": "balanced"}`. For extreme imbalances, Deep Learning methods like `method="ctgan"` usually provide better stability than tree-based methods.*
---
# New Methods Documentation for REAL_GENERATOR_REFERENCE.md

## Content to Add After Existing Methods Section

---

### `ddpm` - Synthcity TabDDPM (Advanced Tabular Diffusion)

**Type:** Deep Learning (Diffusion Model)  
**Best For:** High-quality tabular synthesis, production environments, large datasets  
**Requirements:** `synthcity` (included in base installation)

#### Description

TabDDPM (Tabular Denoising Diffusion Probabilistic Model) is Synthcity's advanced implementation of diffusion models for tabular data. It offers multiple architectures, advanced schedulers, and superior quality compared to the custom `diffusion` method.

#### When to Use

‚úÖ **Use `ddpm` when:**
- You need **maximum quality** synthetic data
- Working with **large datasets** (>100k rows)
- In **production environments** requiring robust, maintained code
- You need **advanced architectures** (ResNet, TabNet)
- You want **cosine scheduling** for better diffusion
- You have **time for longer training** (1000 epochs default)

‚ùå **Don't use `ddpm` when:**
- You need **quick prototyping** (use `diffusion` instead)
- Working with **very small datasets** (<1k rows)
- You have **limited computational resources**
- You need **custom modifications** to the algorithm

#### Parameters

```python
synth = gen.generate(
    data,
    method='ddpm',
    n_samples=1000,
    
    # Training parameters
    n_iter=1000,                    # Training epochs (default: 1000)
    lr=0.002,                       # Learning rate (default: 0.002)
    batch_size=1024,                # Batch size (default: 1024)
    
    # Diffusion parameters
    num_timesteps=1000,             # Diffusion timesteps (default: 1000)
    scheduler='cosine',             # 'cosine' or 'linear' (default: 'cosine')
    gaussian_loss_type='mse',       # 'mse' or 'kl' (default: 'mse')
    
    # Model architecture
    model_type='mlp',               # 'mlp', 'resnet', or 'tabnet' (default: 'mlp')
    model_params={                  # Architecture-specific parameters
        'n_layers_hidden': 3,
        'n_units_hidden': 256,
        'dropout': 0.0
    },
    
    # Task type
    is_classification=False,        # True for classification tasks
)
```

#### Parameter Details

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `n_iter` | int | 1000 | Number of training epochs |
| `lr` | float | 0.002 | Learning rate for optimizer |
| `batch_size` | int | 1024 | Training batch size |
| `num_timesteps` | int | 1000 | Number of diffusion timesteps |
| `scheduler` | str | `'cosine'` | Beta scheduler: `'cosine'` (recommended) or `'linear'` |
| `gaussian_loss_type` | str | `'mse'` | Loss function: `'mse'` or `'kl'` |
| `model_type` | str | `'mlp'` | Architecture: `'mlp'`, `'resnet'`, or `'tabnet'` |
| `model_params` | dict | See above | Architecture-specific parameters |
| `is_classification` | bool | False | Set to True for classification tasks |

#### Model Types

**MLP (Multi-Layer Perceptron)**
- Best for: General tabular data
- Speed: Fast
- Parameters: `n_layers_hidden`, `n_units_hidden`, `dropout`

**ResNet (Residual Network)**
- Best for: Complex feature relationships
- Speed: Medium
- Parameters: `n_layers_hidden`, `n_units_hidden`, `dropout`

**TabNet**
- Best for: Tabular data with feature importance
- Speed: Slower
- Parameters: Specific to TabNet architecture

#### Comparison: `diffusion` vs `ddpm`

| Aspect | `diffusion` (custom) | `ddpm` (Synthcity) |
|--------|---------------------|-------------------|
| **Speed** | ‚ö° Fast (100 epochs) | üê¢ Slower (1000 epochs) |
| **Quality** | ‚≠ê‚≠ê‚≠ê Good | ‚≠ê‚≠ê‚≠ê‚≠ê Excellent |
| **Architectures** | MLP only | MLP/ResNet/TabNet |
| **Scheduler** | Linear | Cosine/Linear |
| **Batch Size** | 64 | 1024 |
| **Use Case** | Quick prototyping | Production quality |
| **Customization** | Easy to modify | Black box |
| **Maintenance** | Your responsibility | Synthcity team |

#### Usage Examples

**Basic Usage:**
```python
from calm_data_generator import RealGenerator
import pandas as pd

gen = RealGenerator()
synth = gen.generate(
    data,
    method='ddpm',
    n_samples=1000,
    n_iter=500  # Reduce for faster training
)
```

**Classification Task:**
```python
synth = gen.generate(
    data,
    method='ddpm',
    n_samples=1000,
    is_classification=True,
    target_col='label'
)
```

**Advanced Architecture:**
```python
synth = gen.generate(
    data,
    method='ddpm',
    n_samples=1000,
    model_type='resnet',
    model_params={
        'n_layers_hidden': 5,
        'n_units_hidden': 512,
        'dropout': 0.1
    },
    scheduler='cosine',
    n_iter=2000
)
```

---

### `timegan` - TimeGAN (Time Series GAN)

**Type:** Deep Learning (GAN for Time Series)  
**Best For:** Complex temporal patterns, multi-entity time series  
**Requirements:** `synthcity` (included in base installation)

#### Description

TimeGAN (Time-series Generative Adversarial Network) is designed specifically for sequential/temporal data. It learns both temporal dynamics and feature distributions, making it ideal for time series with complex patterns.

#### When to Use

‚úÖ **Use `timegan` when:**
- You have **time series data** with temporal dependencies
- Working with **multi-entity sequences** (e.g., multiple users/sensors)
- You need to preserve **temporal dynamics**
- You have **complex temporal patterns** to learn
- You need **high-quality** time series synthesis

‚ùå **Don't use `timegan` when:**
- You have **simple tabular data** (use `ctgan` or `ddpm` instead)
- Working with **very short sequences** (<10 timesteps)
- You need **fast generation** (use `timevae` instead)
- You have **limited computational resources**

#### Data Requirements

TimeGAN expects data in a specific temporal format:
- **Temporal ordering**: Data must be sorted by time
- **Entity grouping**: If multi-entity, group by entity ID
- **Consistent timesteps**: Regular time intervals preferred

#### Parameters

```python
synth = gen.generate(
    data,
    method='timegan',
    n_samples=100,  # Number of sequences to generate
    
    # Training parameters
    n_iter=1000,                    # Training epochs (default: 1000)
    n_units_hidden=100,             # Hidden units in RNN (default: 100)
    batch_size=128,                 # Batch size (default: 128)
    lr=0.001,                       # Learning rate (default: 0.001)
)
```

#### Parameter Details

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `n_iter` | int | 1000 | Number of training epochs |
| `n_units_hidden` | int | 100 | Number of hidden units in RNN layers |
| `batch_size` | int | 128 | Training batch size |
| `lr` | float | 0.001 | Learning rate for optimizer |

#### Usage Examples

**Basic Time Series:**
```python
from calm_data_generator import RealGenerator
import pandas as pd

# Data must have temporal structure
# Example: sensor readings over time
gen = RealGenerator()
synth = gen.generate(
    time_series_data,
    method='timegan',
    n_samples=100,  # Generate 100 sequences
    n_iter=1000,
    n_units_hidden=100
)
```

**Multi-Entity Time Series:**
```python
# Data with multiple entities (e.g., users, sensors)
# Ensure data is sorted by entity_id and timestamp
synth = gen.generate(
    multi_entity_data,
    method='timegan',
    n_samples=50,  # Generate 50 entity sequences
    n_iter=2000,
    n_units_hidden=150,
    batch_size=64
)
```

---

### `timevae` - TimeVAE (Time Series VAE)

**Type:** Deep Learning (VAE for Time Series)  
**Best For:** Regular time series, faster training than TimeGAN  
**Requirements:** `synthcity` (included in base installation)

#### Description

TimeVAE is a variational autoencoder designed for temporal data. It's generally faster than TimeGAN and works well for regular time series with consistent patterns.

#### When to Use

‚úÖ **Use `timevae` when:**
- You have **regular time series** data
- You need **faster training** than TimeGAN
- Working with **consistent temporal patterns**
- You want **good quality** with **less computation**
- You have **moderate-length sequences**

‚ùå **Don't use `timevae` when:**
- You have **highly irregular** time series
- You need **maximum quality** (use `timegan` instead)
- Working with **very complex** temporal dynamics
- You have **simple tabular data** (use `ctgan` or `ddpm`)

#### Data Requirements

Similar to TimeGAN:
- **Temporal ordering**: Data sorted by time
- **Regular intervals**: Works best with consistent timesteps
- **Entity grouping**: If multi-entity, group by entity ID

#### Parameters

```python
synth = gen.generate(
    data,
    method='timevae',
    n_samples=100,  # Number of sequences to generate
    
    # Training parameters
    n_iter=1000,                    # Training epochs (default: 1000)
    decoder_n_layers_hidden=2,      # Decoder layers (default: 2)
    decoder_n_units_hidden=100,     # Decoder units (default: 100)
    batch_size=128,                 # Batch size (default: 128)
    lr=0.001,                       # Learning rate (default: 0.001)
)
```

#### Parameter Details

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `n_iter` | int | 1000 | Number of training epochs |
| `decoder_n_layers_hidden` | int | 2 | Number of hidden layers in decoder |
| `decoder_n_units_hidden` | int | 100 | Number of hidden units in decoder |
| `batch_size` | int | 128 | Training batch size |
| `lr` | float | 0.001 | Learning rate for optimizer |

#### Comparison: `timegan` vs `timevae`

| Aspect | `timegan` | `timevae` |
|--------|-----------|-----------|
| **Speed** | üê¢ Slower | ‚ö° Faster |
| **Quality** | ‚≠ê‚≠ê‚≠ê‚≠ê Excellent | ‚≠ê‚≠ê‚≠ê Good |
| **Complexity** | Handles complex patterns | Best for regular patterns |
| **Training Time** | Longer | Shorter |
| **Use Case** | Complex temporal dynamics | Regular time series |

#### Usage Examples

**Basic Time Series:**
```python
from calm_data_generator import RealGenerator
import pandas as pd

gen = RealGenerator()
synth = gen.generate(
    time_series_data,
    method='timevae',
    n_samples=100,
    n_iter=500,  # Faster than TimeGAN
    decoder_n_units_hidden=100
)
```

**Faster Training:**
```python
# Reduce parameters for quick prototyping
synth = gen.generate(
    time_series_data,
    method='timevae',
    n_samples=50,
    n_iter=300,
    decoder_n_layers_hidden=1,
    decoder_n_units_hidden=50,
    batch_size=64
)
```

---

## Method Selection Guide

### For Tabular Data

| Scenario | Recommended Method | Alternative |
|----------|-------------------|-------------|
| **Quick prototyping** | `diffusion` | `cart`, `rf` |
| **Production quality** | `ddpm` | `ctgan` |
| **Large datasets (>100k)** | `ddpm`, `lgbm` | `ctgan` |
| **Small datasets (<1k)** | `cart`, `rf` | `diffusion` |
| **Class imbalance** | `smote`, `adasyn` | `ctgan` |
| **Preserve correlations** | `ctgan`, `ddpm` | `copula` |
| **Fast generation** | `cart`, `diffusion` | `rf` |
| **Maximum quality** | `ddpm` (ResNet) | `ctgan` |

### For Time Series Data

| Scenario | Recommended Method | Alternative |
|----------|-------------------|-------------|
| **Complex temporal patterns** | `timegan` | - |
| **Regular time series** | `timevae` | `timegan` |
| **Fast training** | `timevae` | - |
| **Multi-entity sequences** | `timegan` | `timevae` |
| **Maximum quality** | `timegan` | `timevae` |

### For Special Cases

| Data Type | Recommended Method |
|-----------|-------------------|
| **Single-cell RNA-seq** | `scvi` |
| **Clinical/Medical** | Use `ClinicalDataGenerator` |
| **Streaming data** | Use `StreamGenerator` |
| **Block/Batch data** | Use `RealBlockGenerator` |
